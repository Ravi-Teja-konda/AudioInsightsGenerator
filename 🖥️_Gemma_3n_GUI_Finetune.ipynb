{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ravi-Teja-konda/AudioInsightsGenerator/blob/main/%F0%9F%96%A5%EF%B8%8F_Gemma_3n_GUI_Finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "YXMvuMnmSIHP"
      },
      "source": [
        "# Fine-tuning Gemma-3n Vision-Language Model for GUI Grounding\n",
        "\n",
        "![banner](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Gemma3n_Wagtial_RD2-V01.original.jpg)\n",
        "\n",
        "This notebook demonstrates how to fine-tune the Gemma-3n vision-language model on the ScreenSpot dataset using TRL (Transformers Reinforcement Learning) with PEFT (Parameter Efficient Fine-Tuning) techniques.\n",
        "\n",
        "## Overview\n",
        "- **Model**: `google/gemma-3n-E2B-it`\n",
        "- **Dataset**: `rootsautomation/ScreenSpot`\n",
        "- **Task**: Training the model to locate GUI elements in screenshots based on text instructions\n",
        "- **Technique**: LoRA (Low-Rank Adaptation) for efficient fine-tuning\n",
        "\n",
        "## About ScreenSpot Dataset\n",
        "The ScreenSpot dataset is a GUI grounding benchmark containing over 1,200 instructions from iOS, Android, macOS, Windows, and Web environments. It's designed to evaluate large multimodal models on GUI grounding tasks - finding specific UI elements in screenshots based on natural language instructions.\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "249BNbx1SIHQ"
      },
      "source": [
        "## 1. Install Dependencies\n",
        "\n",
        "First, let's install all required packages:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kA5OS_UESIHR"
      },
      "outputs": [],
      "source": [
        "!pip install -U torch torchvision torchaudio\n",
        "!pip install transformers datasets accelerate\n",
        "!pip install trl peft bitsandbytes\n",
        "!pip install huggingface_hub pillow\n",
        "!pip install wandb  # Optional: for experiment tracking\n",
        "!pip install -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -U git+https://github.com/huggingface/pytorch-image-models.git"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "NL8mYwFASIHR"
      },
      "source": [
        "## 2. Import Required Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zah1uqL9SIHR"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import hf_hub_download, list_repo_files\n",
        "from PIL import Image\n",
        "from transformers import (\n",
        "    AutoModelForImageTextToText,\n",
        "    AutoProcessor,\n",
        "    Gemma3nForConditionalGeneration,\n",
        ")\n",
        "\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "from peft import LoraConfig\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "Nq4x3jkPSIHS"
      },
      "source": [
        "## 3. Configuration Setup\n",
        "\n",
        "Let's define all the training parameters that were specified in your command:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBHVwIgRSIHS"
      },
      "outputs": [],
      "source": [
        "# Model and training parameters\n",
        "MODEL_NAME = \"google/gemma-3n-E2B-it\"\n",
        "DATASET_NAME = \"rootsautomation/ScreenSpot\"\n",
        "OUTPUT_DIR = \"gemma-3n-E2B-it-trl-sft-screenspot\""
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "QJyqmDJFSIHS"
      },
      "source": [
        "## 4. Helper Functions\n",
        "\n",
        "Define the helper functions for ScreenSpot dataset processing:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaHO2oajSIHS"
      },
      "outputs": [],
      "source": [
        "def format_screenspot_data(samples: dict) -> dict:\n",
        "    \"\"\"Format ScreenSpot dataset to match expected message format\"\"\"\n",
        "    formatted_samples = {\"messages\": []}\n",
        "    for idx in range(len(samples[\"image\"])):\n",
        "        image = samples[\"image\"][idx].convert(\"RGB\")\n",
        "\n",
        "        # Handle different possible field names for instruction\n",
        "        instruction = None\n",
        "        if \"instruction\" in samples:\n",
        "            instruction = samples[\"instruction\"][idx]\n",
        "        elif \"text\" in samples:\n",
        "            instruction = samples[\"text\"][idx]\n",
        "        elif \"query\" in samples:\n",
        "            instruction = samples[\"query\"][idx]\n",
        "\n",
        "        # Handle different possible field names for target/answer\n",
        "        target = None\n",
        "        if \"target\" in samples:\n",
        "            target = str(samples[\"target\"][idx])\n",
        "        elif \"answer\" in samples:\n",
        "            target = str(samples[\"answer\"][idx])\n",
        "        elif \"location\" in samples:\n",
        "            target = str(samples[\"location\"][idx])\n",
        "        elif \"coordinates\" in samples:\n",
        "            target = str(samples[\"coordinates\"][idx])\n",
        "\n",
        "        # If no explicit target, use the instruction as a grounding task\n",
        "        if target is None:\n",
        "            target = \"I'll help you locate that element in the screenshot.\"\n",
        "\n",
        "        message = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"type\": \"text\",\n",
        "                        \"text\": \"You are a helpful assistant that can identify and locate GUI elements in screenshots.\",\n",
        "                    }\n",
        "                ],\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": image},\n",
        "                    {\n",
        "                        \"type\": \"text\",\n",
        "                        \"text\": instruction,\n",
        "                    },\n",
        "                ],\n",
        "            },\n",
        "            {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": target}]},\n",
        "        ]\n",
        "        formatted_samples[\"messages\"].append(message)\n",
        "    return formatted_samples\n",
        "\n",
        "\n",
        "def process_vision_info(messages: list) -> list:\n",
        "    \"\"\"Extract images from message content\"\"\"\n",
        "    image_inputs = []\n",
        "    for msg in messages:\n",
        "        content = msg.get(\"content\", [])\n",
        "        if not isinstance(content, list):\n",
        "            content = [content]\n",
        "\n",
        "        for element in content:\n",
        "            if isinstance(element, dict) and (\n",
        "                \"image\" in element or element.get(\"type\") == \"image\"\n",
        "            ):\n",
        "                if \"image\" in element:\n",
        "                    image = element[\"image\"]\n",
        "                else:\n",
        "                    image = element\n",
        "                if image is not None:\n",
        "                    # Handle dictionary with bytes\n",
        "                    if isinstance(image, dict) and \"bytes\" in image:\n",
        "                        pil_image = Image.open(io.BytesIO(image[\"bytes\"]))\n",
        "                        image_inputs.append(pil_image.convert(\"RGB\"))\n",
        "                    # Handle PIL Image objects\n",
        "                    elif hasattr(image, \"convert\"):\n",
        "                        image_inputs.append(image.convert(\"RGB\"))\n",
        "    return image_inputs\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "mCHMzNo7SIHS"
      },
      "source": [
        "## 5. Model and Processor Setup\n",
        "\n",
        "Load the model and processor with the specified configuration:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "3a8fa9d0fe394654b71e030450f265f6",
            "cc7e7d25c92247ccaa70dd8e15dad43c",
            "a82d6ff595fa40d38a5706709c2158fe",
            "31a02de8d85449ef9360ca19aea01c26",
            "4f2939ca73044407a8da8ee1b961637a",
            "030306646d71452ea7090859088bc669",
            "0cd27851f6404431865e4760c0d27297",
            "2870e7eb1e714c1f96a91fe28f944982",
            "7f6227dadc114ddaaa25191fe3b5fe13",
            "a6f28ea66bad40acbd8efcf92b405851",
            "7ac3f43c35fd449db306ae98fd6edc56"
          ]
        },
        "id": "cp3Tsu2GSIHS",
        "outputId": "cbba0379-d2ef-4fcc-e85f-d2469277dee1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a8fa9d0fe394654b71e030450f265f6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load processor and model\n",
        "processor = AutoProcessor.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "processor.tokenizer.padding_side = \"right\"\n",
        "\n",
        "model = Gemma3nForConditionalGeneration.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    trust_remote_code=True,\n",
        "    # torch_dtype=torch.bfloat16, # Removed\n",
        "    attn_implementation=\"eager\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXG5wkajVXEC"
      },
      "source": [
        "## 6. Dataset Loading and Processing\n",
        "\n",
        "Load and format the ScreenSpot dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BebZk68TSIHT",
        "outputId": "b2f5295e-94c7-45da-b386-5913831a3211"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    test: Dataset({\n",
              "        features: ['file_name', 'bbox', 'instruction', 'data_type', 'data_source', 'image'],\n",
              "        num_rows: 1272\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load dataset\n",
        "dataset = load_dataset(DATASET_NAME)\n",
        "dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70KwOhd8SIHT",
        "outputId": "67e4141e-bcba-4799-906d-1f7a5a2265c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset formatted for training\n"
          ]
        }
      ],
      "source": [
        "# Format the dataset for training\n",
        "formatted_dataset = dataset.map(\n",
        "    format_screenspot_data, batched=True, batch_size=4, num_proc=4\n",
        ")\n",
        "print(\"Dataset formatted for training\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "duwJ2zCMSIHT"
      },
      "source": [
        "## 7. Data Collator Function\n",
        "\n",
        "Define the data collator for processing batches:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6y3O9VjSIHT"
      },
      "outputs": [],
      "source": [
        "def collate_fn(examples):\n",
        "    \"\"\"Custom collate function for processing vision-language data\"\"\"\n",
        "    texts = []\n",
        "    images_list = []\n",
        "\n",
        "    for example in examples:\n",
        "        # Apply chat template to get text\n",
        "        text = processor.apply_chat_template(\n",
        "            example[\"messages\"], tokenize=False, add_generation_prompt=False\n",
        "        ).strip()\n",
        "        texts.append(text)\n",
        "\n",
        "        # Extract images\n",
        "        if \"images\" in example:  # single-image case\n",
        "            images = [img.convert(\"RGB\") for img in example[\"images\"]]\n",
        "        else:  # multi-image case or intersection dataset\n",
        "            images = process_vision_info(example[\"messages\"])\n",
        "        images_list.append(images)\n",
        "\n",
        "    # Tokenize the texts and process the images\n",
        "    batch = processor(text=texts, images=images_list, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    # The labels are the input_ids, and we mask the padding tokens\n",
        "    labels = batch[\"input_ids\"].clone()\n",
        "\n",
        "    # Use Gemma3n specific token masking\n",
        "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
        "    if hasattr(processor.tokenizer, \"image_token_id\"):\n",
        "        labels[labels == processor.tokenizer.image_token_id] = -100\n",
        "    if hasattr(processor.tokenizer, \"boi_token_id\"):\n",
        "        labels[labels == processor.tokenizer.boi_token_id] = -100\n",
        "    if hasattr(processor.tokenizer, \"eoi_token_id\"):\n",
        "        labels[labels == processor.tokenizer.eoi_token_id] = -100\n",
        "\n",
        "    batch[\"labels\"] = labels\n",
        "    return batch\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "RlOnDPj0SIHT"
      },
      "source": [
        "## 8. Training Configuration\n",
        "\n",
        "Set up the training arguments and PEFT configuration:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVgRL_kpSIHT"
      },
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "training_args = SFTConfig(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=1e-4,\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    weight_decay=0.01,\n",
        "    max_grad_norm=1.0,\n",
        "    bf16=True,\n",
        "    remove_unused_columns=False,\n",
        "    gradient_checkpointing=False,\n",
        "    dataloader_pin_memory=False,\n",
        "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "rjfJZFbVSIHT"
      },
      "source": [
        "## 9. Initialize Trainer\n",
        "\n",
        "Create the SFTTrainer with all configurations:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4PH4W5LSIHT",
        "outputId": "7a60a2cf-d6fa-4a7c-cc84-9d97cb9c4969"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ],
      "source": [
        "# Initialize trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=collate_fn,\n",
        "    train_dataset=formatted_dataset[\"test\"],\n",
        "    processing_class=processor.tokenizer,\n",
        "    peft_config=peft_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "jDcbijJ7SIHT"
      },
      "source": [
        "## 10. Training\n",
        "\n",
        "Start the fine-tuning process:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FyNKcvMSIHT",
        "outputId": "5186aa54-9104-4774-a5ae-021f1ae733b8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='477' max='477' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [477/477 3:38:35, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>33.369700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>5.756200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>4.718700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>5.325900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>4.515500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>4.469500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>3.972500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>4.436800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>4.352000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>4.519800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>3.901000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>4.098800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>3.946100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>3.487300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>3.501600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>3.584600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>3.771100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>3.164200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>3.551900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>3.354300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>3.507300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>3.528700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>3.200500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>3.010800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>3.252000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>3.222300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>3.371100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>3.571600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>3.451600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>3.392800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>3.458800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>3.619300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>3.293900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>3.393300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>3.263000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>3.088400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>3.183300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>2.888900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>2.774000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>2.935500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>3.269600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>2.938100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>2.961700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>3.015400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>2.809400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>3.083100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>3.077300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=477, training_loss=4.201079038703966, metrics={'train_runtime': 13127.7412, 'train_samples_per_second': 0.291, 'train_steps_per_second': 0.036, 'total_flos': 1.9970850051072e+16, 'train_loss': 4.201079038703966})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "RbyXNC_vSIHT"
      },
      "source": [
        "## 11. Save Model\n",
        "\n",
        "Save the fine-tuned model and processor:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MtCytoaSIHT"
      },
      "outputs": [],
      "source": [
        "# Save model and processor\n",
        "trainer.save_model(training_args.output_dir)\n",
        "processor.save_pretrained(training_args.output_dir)\n",
        "print(f\"Model saved to: {training_args.output_dir}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "MVysMZ6kSIHT"
      },
      "source": [
        "## 12. Optional: Test the Fine-tuned Model\n",
        "\n",
        "Let's test the model on a sample from the dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwJH5eFnSIHU"
      },
      "outputs": [],
      "source": [
        "# Get test sample\n",
        "test_sample = formatted_dataset[\"test\"][0]\n",
        "test_image = None\n",
        "test_question = None\n",
        "expected_answer = None\n",
        "\n",
        "for message in test_sample[\"messages\"]:\n",
        "    if message[\"role\"] == \"user\":\n",
        "        for content in message[\"content\"]:\n",
        "            if content[\"type\"] == \"image\":\n",
        "                test_image = content[\"image\"]\n",
        "            elif content[\"type\"] == \"text\":\n",
        "                test_question = content[\"text\"]\n",
        "    elif message[\"role\"] == \"assistant\":\n",
        "        expected_answer = message[\"content\"][0][\"text\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npYQaaXDSIHU"
      },
      "outputs": [],
      "source": [
        "# Display test sample\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(f\"Question: {test_question}\")\n",
        "print(f\"Expected answer: {expected_answer}\")\n",
        "\n",
        "if test_image is not None:\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(test_image)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Test Image\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JDBEdJISIHU"
      },
      "outputs": [],
      "source": [
        "# Generate prediction\n",
        "if test_image is not None:\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": \"You are a helpful assistant that can identify and locate GUI elements in screenshots.\",\n",
        "                }\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": test_image},\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": test_question,\n",
        "                },\n",
        "            ],\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    text = processor.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "    inputs = processor(text=text, images=[test_image], return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=100,\n",
        "            do_sample=False,\n",
        "            temperature=0.1,\n",
        "        )\n",
        "\n",
        "    response = processor.decode(outputs[0], skip_special_tokens=True)\n",
        "    generated_text = response[len(text) :].strip()\n",
        "\n",
        "    print(f\"Model prediction: {generated_text}\")\n",
        "    print(f\"Expected: {expected_answer}\")\n",
        "    print(f\"Match: {generated_text.strip() == expected_answer.strip()}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "IynfMlbtSIHU"
      },
      "source": [
        "## 13. Optional: Push to Hub\n",
        "\n",
        "If you want to share your fine-tuned model, you can push it to the Hugging Face Hub:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dyr6z6sSIHU"
      },
      "outputs": [],
      "source": [
        "# Optional: Push to Hugging Face Hub\n",
        "# hub_model_id = \"your-username/gemma-3n-screenspot-finetuned\"\n",
        "# trainer.push_to_hub(hub_model_id)\n",
        "# processor.push_to_hub(hub_model_id)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "fDumV_TGSIHU"
      },
      "source": [
        "## Summary\n",
        "\n",
        "Congratulations! You have successfully:\n",
        "\n",
        "1. ✅ Loaded the Gemma-3n vision-language model\n",
        "2. ✅ Prepared the ScreenSpot dataset for training\n",
        "3. ✅ Configured PEFT/LoRA for efficient fine-tuning\n",
        "4. ✅ Fine-tuned the model on GUI grounding tasks\n",
        "5. ✅ Saved the trained model\n",
        "6. ✅ Tested the model on sample data\n",
        "\n",
        "### Key Configuration Used:\n",
        "- **Model**: `google/gemma-3n-E2B-it`\n",
        "- **Dataset**: `rootsautomation/ScreenSpot`\n",
        "- **LoRA Rank**: 8 with Alpha 16\n",
        "- **Target Modules**: q_proj, v_proj\n",
        "- **Learning Rate**: 1e-4 with cosine scheduling\n",
        "- **Epochs**: 3\n",
        "\n",
        "### Next Steps:\n",
        "- Evaluate the model on more GUI grounding samples\n",
        "- Experiment with different LoRA configurations\n",
        "- Try fine-tuning on other vision-language tasks\n",
        "- Test on different types of GUI interfaces (iOS, Android, Web, etc.)\n",
        "- Share your model on the Hub for others to use\n",
        "\n",
        "The fine-tuned model is now ready for inference on GUI grounding tasks!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d32c7c5",
        "outputId": "dae2f038-0dbc-4e81-83c3-a621ec1c3341"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "173"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "del model\n",
        "del trainer\n",
        "del processor\n",
        "\n",
        "# Clear CUDA cache\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Collect garbage\n",
        "gc.collect()\n",
        "\n",
        "# Optional: Delete model and trainer if they are large\n",
        "# del model\n",
        "# del trainer\n",
        "# gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAi55YsVcM3t"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "030306646d71452ea7090859088bc669": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cd27851f6404431865e4760c0d27297": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2870e7eb1e714c1f96a91fe28f944982": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31a02de8d85449ef9360ca19aea01c26": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6f28ea66bad40acbd8efcf92b405851",
            "placeholder": "​",
            "style": "IPY_MODEL_7ac3f43c35fd449db306ae98fd6edc56",
            "value": " 3/3 [00:07&lt;00:00,  2.48s/it]"
          }
        },
        "3a8fa9d0fe394654b71e030450f265f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cc7e7d25c92247ccaa70dd8e15dad43c",
              "IPY_MODEL_a82d6ff595fa40d38a5706709c2158fe",
              "IPY_MODEL_31a02de8d85449ef9360ca19aea01c26"
            ],
            "layout": "IPY_MODEL_4f2939ca73044407a8da8ee1b961637a"
          }
        },
        "4f2939ca73044407a8da8ee1b961637a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ac3f43c35fd449db306ae98fd6edc56": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f6227dadc114ddaaa25191fe3b5fe13": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a6f28ea66bad40acbd8efcf92b405851": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a82d6ff595fa40d38a5706709c2158fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2870e7eb1e714c1f96a91fe28f944982",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f6227dadc114ddaaa25191fe3b5fe13",
            "value": 3
          }
        },
        "cc7e7d25c92247ccaa70dd8e15dad43c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_030306646d71452ea7090859088bc669",
            "placeholder": "​",
            "style": "IPY_MODEL_0cd27851f6404431865e4760c0d27297",
            "value": "Loading checkpoint shards: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}